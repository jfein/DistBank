TestPlan: Vera Kutsenko, Jeremy Fein

In order to test our primary back up with did the following:

For all scenarios when we failed a given server, we would fail the server first, then attempt to use the client to make some transaction, to ensure that it will be waiting on the primary since we have not notified of the failed primary. We then notify of the failure, using this we ensured the client then switched to the next available backup and completed the transaction. After this, we would recover at which point no one knows of the recovery yet. Once we notify of the recovery, we made sure that the node is synched with the appropriate data and up as a back up server.

(1) Tested with one client with one primary and backup server. Crashed the node with the primary server and notified those who were subscribed to the node of the failure to ensure that the backup would take over, and then recovered, notified of recovery to make sure the old primary would now be a back up.

Client: Primary Backup
A:	100	101

 Do a transaction into A. Crash 100. Attempt to do another transaction into A, but since 100 failed but we don't know about it yet, A is stuck waiting. Notify failure, A should route its transaction to 101 which becomes new primary. Recover 100, notify of recovery, which will make 100 the new backup. 	 

(2) Tested with two clients and one primary, two backup servers each where the primary of one client serves as a backup of the other, and vice versa. The two clients have two different primaries. We proceeded to do some deposit transactions to make sure we had some data for an account for each client. We then proceeded to fail both primaries for both clients which would mean that the both clients, once notified, will contact the same server who will now be a primary for both clients. We then notified, and checked to make sure the primary was correct.

Scenario described above is demonstrated below:
Client: Primary Backup Backup
A: 	101, 	100, 	102
B: 	100, 	101, 	102

We made deposits into A, B. We then failed 101, 100. Before notifying of the failure, we attempted to do deposit,withdraw, or query transaction to make sure the client will start waiting for a response from the primary that's failed. We then notify of the failure, and then both clients will reroute their transactions to 102, and 102 will serve as the primary for both.

   We then proceeded to recover 101 and 100 to ensure that they became new backups for A and B.

(3) For scenarios (1) & (2) we tested failure of backups only, to make sure that once we notified of recovery for backups, they came back as backups and synched with the other servers.

We did this with withdraw, transfer, and deposit requests.

(4) To test transfer requests we:
	For transfer requests, because of their fast nature, we put sleep statements in different parts of the transfer function in BranchState in order to allow us time to fail a remote node in order to properly test transfer.
	a. Tested failure before making the request to deposit, but after withdrawing.
		This is fine because no state would have been recorded. The client GUI will retry the transaction to a new primary that it will choose from the backups.
	b. Tested failure after making the request to deposit to a remove branch, but before receiving a response from the other branch we are trying to deposit into.
		In this case, the BranchApp now acts as a client. So, the BranchApp will hang just as a client will until it is notified of the remote failure, in which case it will try to do the deposit again on the new primary.
	c. Tested failure after doing local withdraw and a succesful deposit to remote branch, but have not yet responded to the client.
		Here, the remote Deposit has been completed with the original serial number, but the withdraw from the src account has not been recorded or backed up anywhere. Thus, we are in an inconsistent state. However, when this happens, since the Client will get no response from the failed BranchApp, it will try to do the Transfer request again with a new primary. This new primary will then do the withdraw and re-attempt the Deposit. When this happens, sicne the serial number was already used, the Deposit will not be commited twice and the new primary will think the whole transfer transaction is finished. It will then synch with the backups and send a response to the client. Thus, this scneario is OK.

(5) Tested synch requests:

  Client: Primary Backup Backup
  A:	  100	   101 	 102

	a. Synch Requests: We had a client, A, with one primary, 100, and two backups, 101 and 102.
		i. When A made a transaction, we considered the scenario if 100 failed after 101 received a synch request with the updated bank state, but before 102 received a synch request.
			- In this case, when A is notified of failure it will send the same request to 101, which will already have the updated state and that transaction. So, 101 will just ignore the transaction, but will still  synch with its backup 102 and then send a response to A. Thus, 101 and 102 are consistent and the transaction succeeded.
		ii. When A made a transaction, we considered the scenario if 100 failed after 102 received a synch request with the updated bank state, but before 101 received a synch request.
			- If A is notified of failure and it sends its request to 101, which doesn't have the current updated state, then 101 will perform the transaction as if it hasnt occured before. It then SYNCHs with 102, who then gets up to date. It doesnt matter that 102 has the transaction in its state, since 101 just performs it again and synchs 102. Thus, this is OK.
		ii. If 100 failed before synching.
			Then A will retry the transaction to either 101 or 102, and everything will be fine.
		iii. If 100 failed after synching to both.
			Then A will retry the transaction to 101 or 102, but since both already saw the transaction, they will ignore it, and the state will stay the same.

(6) Fail, NotifyFail, NotifyRecovery:
	(a) Fail Request:
		When we send a fail request, the node calls sys.exit and dies. 
	(b) NotifyFail Request:
		Once we use the oracle to fail the node, we tested that it was failed by using the clients to make requests. Since, as mentioned previously, the clients haven't been notified of the failure, they should hang. However, once we call notify failure and a NotifyFailure request is sent out, the client should complete. This shows that the request was successfully processed.
	(c ) NotifyRecovery:
		Once we have failed, and notified of the failure, we call recover. This will call the LANCH_ONE.cmd script which will relaunch the node that you select from the oracle gui's drop down menu. When we try to make transactions before notifying the recovery, we observed whether the recovered node would obtain the syncs. It did not, therefore, after notify recovery, we observed that it did sync on transactions and that the node recovered as backup.
		
